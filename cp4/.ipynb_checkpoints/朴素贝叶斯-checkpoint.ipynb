{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 使用python进行文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1 词表到向量的转换函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]    #1 is abusive, 0 not\n",
    "    return postingList, classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    词集模型\n",
    "    \"\"\"\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1 #对应位置为1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts,listClasses=loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVocabList=createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flea',\n",
       " 'so',\n",
       " 'steak',\n",
       " 'posting',\n",
       " 'park',\n",
       " 'take',\n",
       " 'to',\n",
       " 'is',\n",
       " 'worthless',\n",
       " 'love',\n",
       " 'how',\n",
       " 'dog',\n",
       " 'problems',\n",
       " 'help',\n",
       " 'buying',\n",
       " 'garbage',\n",
       " 'cute',\n",
       " 'I',\n",
       " 'has',\n",
       " 'licks',\n",
       " 'please',\n",
       " 'dalmation',\n",
       " 'my',\n",
       " 'ate',\n",
       " 'quit',\n",
       " 'maybe',\n",
       " 'him',\n",
       " 'food',\n",
       " 'stop',\n",
       " 'not',\n",
       " 'stupid',\n",
       " 'mr']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList,listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList,listOPosts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2 朴素贝叶斯分类器训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    p0Num = np.zeros(numWords)\n",
    "    p1Num = np.zeros(numWords)\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1: #侮辱性的\n",
    "            p1Num += trainMatrix[i] #该类别下各个单词数+1\n",
    "            p1Denom += sum(trainMatrix[i]) #该类别总数+1\n",
    "        else: #不是侮辱性的\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Num / p1Denom  # \n",
    "    p0Vect = p0Num / p0Denom  # \n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat=[]\n",
    "for positionDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,positionDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V,p1V,pAb=trainNB0(trainMat,listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04166667, 0.04166667, 0.04166667, 0.        , 0.        ,\n",
       "       0.        , 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.        ,\n",
       "       0.        , 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.125     , 0.04166667, 0.        ,\n",
       "       0.        , 0.08333333, 0.        , 0.04166667, 0.        ,\n",
       "       0.        , 0.04166667])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.05263158, 0.05263158,\n",
       "       0.05263158, 0.05263158, 0.        , 0.10526316, 0.        ,\n",
       "       0.        , 0.10526316, 0.        , 0.        , 0.05263158,\n",
       "       0.05263158, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05263158,\n",
       "       0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158,\n",
       "       0.15789474, 0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    #防止出现概率为0的情况\n",
    "    p0Num = np.ones(numWords); p1Num = np.ones(numWords)      #change to np.ones()\n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    #防止数值太小，下溢出\n",
    "    p1Vect = np.log(p1Num/p1Denom)          #change to np.log()\n",
    "    p0Vect = np.log(p0Num/p0Denom)          #change to np.log()\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3 朴素贝叶斯分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    #改为对数相加\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "#     print('p1 is{},p0 is {}'.format(p1,p0))\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个人测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'] classified as:  0\n"
     ]
    }
   ],
   "source": [
    "test_entry=['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him']\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, test_entry))\n",
    "print(test_entry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-4 朴素贝叶斯词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 使用朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySent='This book is the best book on Python or M.L. I have ever laid eyes upon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "regEx=re.compile('\\\\W*')#除了单词数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon',\n",
       " '']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfTokens=regEx.split(mySent)\n",
    "listOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.lower() for tok in listOfTokens if len(tok)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailText=open('email/ham/6.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "listOfTokens=regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listOfTokens #查看结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-5 文件解析及完整垃圾邮件测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W+', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def spamTest():\n",
    "    docList = []; classList = []; fullText = []\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    trainingSet = range(50); testSet = []           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(list(trainingSet)[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print(\"classification error\", docList[docIndex])\n",
    "    print('the error rate is: ', float(errorCount)/len(testSet))\n",
    "    #return vocabList, fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n",
      "the error rate is:  0.0\n",
      "the error rate is:  0.0\n",
      "the error rate is:  0.0\n",
      "the error rate is:  0.0\n",
      "the error rate is:  0.0\n",
      "the error rate is:  0.0\n",
      "the error rate is:  0.0\n",
      "the error rate is:  0.0\n",
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.7 使用朴素贝叶斯分类器从个人广告中获取区域倾向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny=feedparser.parse('https://rsshub.app/npc/c183')\n",
    "#各种rss源\n",
    "#https://docs.rsshub.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ny['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '全过程人民民主在民法典编纂中的生动体现',\n",
       " 'title_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': 'https://rsshub.app/npc/c183',\n",
       "  'value': '全过程人民民主在民法典编纂中的生动体现'},\n",
       " 'summary': '<p style=\"text-indent: 2em;\">习近平总书记在2021年10月召开的中央人大工作会议上明确指出，我国全过程人民民主是全链条、全方位、全覆盖的民主，是最广泛、最真实、最管用的社会主义民主。这一重大理念，深刻揭示了我国社会主义民主的政治内涵、理论内涵，为我国发展社会主义民主提供了根本遵循。</p><p style=\"text-indent: 2em;\">全过程人民民主与社会主义法治建设特别是立法工作息息相关，与以人民为中心的发展思想息息相关，与人民日益增长的美好生活需要和群众关心关注的实际问题息息相关。民法典作为新中国第一部以法典命名的法律，在编纂过程中充分践行以人民为中心的发展思想，立足发展全过程人民民主，充分听取广大人民群众的意见，积极回应人民群众的关切，确保编纂出一部符合人民意愿、保障人民民事权利的权利法典。</p><p style=\"text-indent: 2em;\"><span>立足全过程人民民主<span style=\"font-size: 12pt;\">的本质属性</span></span></p><p style=\"text-indent: 2em;\">全过程人民民主的本质属性，在于广泛的人民性。民法典调整民事主体的人身关系、财产关系，涉及每个人、每个家庭、每个企业、每个组织，是全方位保障人民民事权利的权利法典。习近平总书记在中央政治局第二十次集体学习时指出，民法典是“一部体现对生命健康、财产安全、交易便利、生活幸福、人格尊严等各方面权利平等保护的民法典”。编纂好这样一部权利法典，必须立足人民属性，坚持以人民为中心的立法理念，一方面突出保护民事权利这一主线，做好民事权利制度的顶层设计，强化对民事主体人身权利、财产权利的全方位保护，形成规范有效的权利保护体系；另一方面对社会公众比较关注的问题作出有针对性的规定，真正做到“民有所呼、法有所应”，推动民事立法的精细化，强化法律的适用性。例如，目前网上购物已经成为公众喜爱的消费方式，为此，民法典的合同编对电子合同的订立、履行规则作出了有针对性的规定；再如，近年来高空抛物坠物事件时有发生，成为社会关注群众热议的问题。为此，民法典对高空抛物坠物责任规则作了进一步的完善，依法保障人民群众的“头顶安全”，等等。坚持以人民为中心的立法理念，积极回应人民群众的期待和诉求，终将赢得人民的拥护。民法典的高票通过反映了广大人大代表对民法典坚持人民属性的高度认可，也是全过程人民民主实实在在的成果。</p><p style=\"text-indent: 2em;\"><span>坚持全过程人民民主<span style=\"font-size: 12pt;\">的制度保障</span></span></p><p style=\"text-indent: 2em;\">习近平总书记强调，人民代表大会制度是实现我国全过程人民民主的重要制度载体。人大代表作为国家权力机关的组成人员，代表人民行使立法权。栗战书委员长多次指出要充分发挥代表作用。在民法典编纂工作中，我们认真贯彻栗战书委员长的这一指示要求，认真听取、吸收代表的意见。</p><p style=\"text-indent: 2em;\">一是认真梳理全国人大代表多年来就编纂民法典提出的议案建议，特别是对2015年至2019年期间的179件议案建议进行重点研究，综合分析形成简报，作为工作中的重要参考。同时，对于全国人大代表提出的有关民法典的议案和建议认真办理，准确把握代表议案建议中的核心内涵，并尽可能予以吸收采纳。经统计，民法典共吸收了40多位全国人大代表的议案建议。</p><p style=\"text-indent: 2em;\">二是在立法调研、座谈中，我们积极邀请有关全国人大代表参加，当面听取他们的意见，并进行适当的解释沟通，增进他们对编纂工作的理解。经统计，在编纂工作中我们共邀请了20余位全国人大代表参加立法调研、座谈活动。</p><p style=\"text-indent: 2em;\">三是认真组织代表研读，充分听取代表意见。在民法典编纂过程中，我们先后三次组织代表研读草案:第一次是2017年1月，在十二届全国人大五次会议审议民法总则草案前，组织全国人大代表进行研读。第二次是2019年12月，在十三届全国人大三次会议审议民法典草案前组织代表研读。第三次是2020年4月，这是新增的一次代表研读。由于突如其来的新冠肺炎疫情，十三届全国人大三次会议会期缩短，而民法典草案条文众多，可以说时间紧、任务重。为进一步做好准备工作，更充分听取全国人大代表的意见，法工委将修改后的民法典草案再次发送给各省、自治区、直辖市人大常委会，请各地以适当方式组织有关全国人大代表研读讨论，再次听取代表意见。在民法典编纂的五年时间里，先后共13轮次将民法典相关法律草案印发代表征求意见，通过听取并充分吸收代表们的意见，凝聚代表们的最大共识。</p><p style=\"text-indent: 2em;\">四是在常委会和大会的审议中，对常委会组成人员、人大代表提出的意见和建议给予高度重视，需要解释的及时作出解释，能够吸收的尽量吸收。代表大会审议民法典草案期间，共整理1241位代表提出的2956条意见，宪法法律委、法工委认真听取代表们提出的审议意见，并积极予以采纳，对民法典草案作了100多处修改，其中实质性修改达到40多处，得到了广大代表的认可。</p><p style=\"text-indent: 2em;\">重视发挥人大代表在立法中的重要作用，充分尊重代表所提意见的不懈努力结出了丰硕成果：2017年3月15日，第十二届全国人大第五次会议以2782票、98.3%的赞成率通过了民法总则，顺利完成民法典编纂第一步；2020年5月28日，第十三届全国人大第三次会议以2879票、99.8%的赞成率通过了民法典。这是充分发挥代表作用、发扬人民代表大会制度优势所取得的重大成果。</p><p style=\"text-indent: 2em;\"><span>依托全过程人民民主<span style=\"font-size: 12pt;\">的深厚基础</span></span></p><p style=\"text-indent: 2em;\">全过程人民民主的深厚基础在于，要让人民广泛而深入地参与到民主协商、民主决策、民主管理和民主监督过程中，切实保证人民当家作主。民主立法的实践依托于这样的深厚基础。在民法典的编纂工作中，我们通过认真开展调查研究，广泛倾听民意，践行民主立法要求，将开门立法、问法于民贯穿于编纂工作的全过程。</p><p style=\"text-indent: 2em;\">五年来，共10次通过中国人大网公开征求社会公众意见，有42万多群众参与，提出102万多条意见，对这些意见，我们都进行认真梳理研究，并积极予以采纳。例如，根据公众的意见，增加了禁止高利放贷的规定，完善了预防、制止性骚扰的规定，增加了夫妻共同债务认定的规定，增加规定了收养评估制度等。</p><p style=\"text-indent: 2em;\">五年来，共27次赴地方开展立法调研，广鉴民情、汇聚民意、集中民智。我们还多次将草案印发中央有关部门、地方人大、有关研究机构、基层立法联系点等征求意见。</p><p style=\"text-indent: 2em;\">从调研和听取意见的方式上看：一是调研深入基层、深入群众，借助法工委设立的基层立法联系点，广泛听取基层群众的意见。编纂工作五年来，上海虹桥、江西景德镇、湖北襄阳、甘肃临洮这些基层立法联系点，作为国家立法机关联系基层群众的桥梁和纽带，都留下了常委会领导、法工委同志调研的足迹。对基层立法联系点提出的意见，我们都认真研究，积极采纳。如在湖北襄阳市人大常委会调研民法典婚姻家庭编时，他们提出了增加规定夫妻共同债务认定和收养评估制度的建议，得到了吸收采纳。二是综合调研与专题调研相结合，确保调研的针对性和实效性。不仅有全面了解情况的综合调研，还有专门针对农村集体经济组织的民事主体地位、夫妻共同债务、保理合同等各编重点问题的专题调研。三是委托专门机构开展专项调查。针对社会公众关心的法定婚龄问题，我们专门委托国家统计局社情民意调查中心对公民的结婚意向年龄开展调查，借助专业力量为立法决策提供了更为广泛的民意基础。四是在立法调研中积极采用问卷调查的方式，进一步提高调研质量。我们在广西、北京、浙江进行的三次调研中，针对居民小区管理、婚姻家庭继承制度等群众关心的问题，主动运用问卷调查配合开展调研，拓宽了解情况、收集意见的途径，取得了积极效果。</p><p style=\"text-indent: 2em;\"><span>展现全过程人民民主<span style=\"font-size: 12pt;\">的务实风范</span></span></p><p style=\"text-indent: 2em;\">习近平总书记指出：“民主不是装饰品，不是用来做摆设的，而是要用来解决人民需要解决的问题的。”全过程人民民主是真正的民主，是管用的民主，是回答现实问题、回应社会关切的民主。</p><p style=\"text-indent: 2em;\">随着经济社会的发展，人们的生产生活方式发生了深刻变化，给民事法律制度的发展完善带来了新的课题。在编纂民法典过程中，我们认真贯彻党中央的决策部署，深入分析研究法治建设中的新问题，积极回应人民群众的新期待，回答时代发展提出的新课题。例如，为解决民间借贷领域存在的突出问题，维护正常的金融秩序，在合同编中增加禁止高利放贷的规定；为解决人民群众反映强烈的小区管理问题，对物权编中的业主建筑物区分所有权制度作了进一步的完善，并在合同编中增加“物业服务合同”这一新的典型合同；为回应实践中出现的基因编辑，以及由于人工智能技术的发展带来的“换脸”“换声”等问题，在人格权编中增加了相应规定；为解决互联网时代的网络侵权问题，对相关侵权责任规定进行了完善等。</p><p style=\"text-indent: 2em;\">对社会公众比较关注的一些问题，如涉及婚姻家庭编的有关问题，我们也高度重视，通过法工委发言人制度平台，及时对外发声，回应关切，以解疑释惑，凝聚共识，为民法典编纂工作平稳顺利推进营造良好的舆论环境。2019年，通过法工委发言人平台，我们对媒体关注的有关热点问题作了积极回应，取得了很好的社会效果。2020年新冠肺炎疫情发生以来，我们认真学习贯彻习近平总书记的有关重要讲话精神，结合民法典编纂工作，一方面对与疫情相关的民事法律制度进行梳理研究，对草案作出有针对性的修改完善；另一方面针对社会关注的不可抗力等热点问题，通过法工委发言人主动发声，回应社会关切。</p><p style=\"text-indent: 2em;\">民法典编纂过程中的这些真招实干，展现出全过程人民民主的务实风范，得到了常委会组成人员、全国人大代表和各方面的总体认同。</p><p style=\"text-indent: 2em;\">2021年8月20日，栗战书委员长在第十三届全国人大常委会第三十次会议上的讲话中明确提出，发展全过程人民民主，是人大及其常委会的重要职责和使命。要在党的领导下，充分发挥人民代表大会这一主要民主渠道作用，把发展全过程人民民主的要求贯彻体现到人大各项工作中。全过程人民民主在民法典编纂工作中的体现，鲜活地阐释着我国全过程人民民主的原则理念、制度安排、生动实践和巨大成就，有助于我们进一步做到理论上清醒、政治上坚定、制度上自信、行动上自觉。民法典的中国故事还在继续，全过程人民民主的中国民主故事将更启新篇。（水淼\\u2002全国人大常委会法工委民法室）</p>',\n",
       " 'summary_detail': {'type': 'text/html',\n",
       "  'language': None,\n",
       "  'base': 'https://rsshub.app/npc/c183',\n",
       "  'value': '<p style=\"text-indent: 2em;\">习近平总书记在2021年10月召开的中央人大工作会议上明确指出，我国全过程人民民主是全链条、全方位、全覆盖的民主，是最广泛、最真实、最管用的社会主义民主。这一重大理念，深刻揭示了我国社会主义民主的政治内涵、理论内涵，为我国发展社会主义民主提供了根本遵循。</p><p style=\"text-indent: 2em;\">全过程人民民主与社会主义法治建设特别是立法工作息息相关，与以人民为中心的发展思想息息相关，与人民日益增长的美好生活需要和群众关心关注的实际问题息息相关。民法典作为新中国第一部以法典命名的法律，在编纂过程中充分践行以人民为中心的发展思想，立足发展全过程人民民主，充分听取广大人民群众的意见，积极回应人民群众的关切，确保编纂出一部符合人民意愿、保障人民民事权利的权利法典。</p><p style=\"text-indent: 2em;\"><span>立足全过程人民民主<span style=\"font-size: 12pt;\">的本质属性</span></span></p><p style=\"text-indent: 2em;\">全过程人民民主的本质属性，在于广泛的人民性。民法典调整民事主体的人身关系、财产关系，涉及每个人、每个家庭、每个企业、每个组织，是全方位保障人民民事权利的权利法典。习近平总书记在中央政治局第二十次集体学习时指出，民法典是“一部体现对生命健康、财产安全、交易便利、生活幸福、人格尊严等各方面权利平等保护的民法典”。编纂好这样一部权利法典，必须立足人民属性，坚持以人民为中心的立法理念，一方面突出保护民事权利这一主线，做好民事权利制度的顶层设计，强化对民事主体人身权利、财产权利的全方位保护，形成规范有效的权利保护体系；另一方面对社会公众比较关注的问题作出有针对性的规定，真正做到“民有所呼、法有所应”，推动民事立法的精细化，强化法律的适用性。例如，目前网上购物已经成为公众喜爱的消费方式，为此，民法典的合同编对电子合同的订立、履行规则作出了有针对性的规定；再如，近年来高空抛物坠物事件时有发生，成为社会关注群众热议的问题。为此，民法典对高空抛物坠物责任规则作了进一步的完善，依法保障人民群众的“头顶安全”，等等。坚持以人民为中心的立法理念，积极回应人民群众的期待和诉求，终将赢得人民的拥护。民法典的高票通过反映了广大人大代表对民法典坚持人民属性的高度认可，也是全过程人民民主实实在在的成果。</p><p style=\"text-indent: 2em;\"><span>坚持全过程人民民主<span style=\"font-size: 12pt;\">的制度保障</span></span></p><p style=\"text-indent: 2em;\">习近平总书记强调，人民代表大会制度是实现我国全过程人民民主的重要制度载体。人大代表作为国家权力机关的组成人员，代表人民行使立法权。栗战书委员长多次指出要充分发挥代表作用。在民法典编纂工作中，我们认真贯彻栗战书委员长的这一指示要求，认真听取、吸收代表的意见。</p><p style=\"text-indent: 2em;\">一是认真梳理全国人大代表多年来就编纂民法典提出的议案建议，特别是对2015年至2019年期间的179件议案建议进行重点研究，综合分析形成简报，作为工作中的重要参考。同时，对于全国人大代表提出的有关民法典的议案和建议认真办理，准确把握代表议案建议中的核心内涵，并尽可能予以吸收采纳。经统计，民法典共吸收了40多位全国人大代表的议案建议。</p><p style=\"text-indent: 2em;\">二是在立法调研、座谈中，我们积极邀请有关全国人大代表参加，当面听取他们的意见，并进行适当的解释沟通，增进他们对编纂工作的理解。经统计，在编纂工作中我们共邀请了20余位全国人大代表参加立法调研、座谈活动。</p><p style=\"text-indent: 2em;\">三是认真组织代表研读，充分听取代表意见。在民法典编纂过程中，我们先后三次组织代表研读草案:第一次是2017年1月，在十二届全国人大五次会议审议民法总则草案前，组织全国人大代表进行研读。第二次是2019年12月，在十三届全国人大三次会议审议民法典草案前组织代表研读。第三次是2020年4月，这是新增的一次代表研读。由于突如其来的新冠肺炎疫情，十三届全国人大三次会议会期缩短，而民法典草案条文众多，可以说时间紧、任务重。为进一步做好准备工作，更充分听取全国人大代表的意见，法工委将修改后的民法典草案再次发送给各省、自治区、直辖市人大常委会，请各地以适当方式组织有关全国人大代表研读讨论，再次听取代表意见。在民法典编纂的五年时间里，先后共13轮次将民法典相关法律草案印发代表征求意见，通过听取并充分吸收代表们的意见，凝聚代表们的最大共识。</p><p style=\"text-indent: 2em;\">四是在常委会和大会的审议中，对常委会组成人员、人大代表提出的意见和建议给予高度重视，需要解释的及时作出解释，能够吸收的尽量吸收。代表大会审议民法典草案期间，共整理1241位代表提出的2956条意见，宪法法律委、法工委认真听取代表们提出的审议意见，并积极予以采纳，对民法典草案作了100多处修改，其中实质性修改达到40多处，得到了广大代表的认可。</p><p style=\"text-indent: 2em;\">重视发挥人大代表在立法中的重要作用，充分尊重代表所提意见的不懈努力结出了丰硕成果：2017年3月15日，第十二届全国人大第五次会议以2782票、98.3%的赞成率通过了民法总则，顺利完成民法典编纂第一步；2020年5月28日，第十三届全国人大第三次会议以2879票、99.8%的赞成率通过了民法典。这是充分发挥代表作用、发扬人民代表大会制度优势所取得的重大成果。</p><p style=\"text-indent: 2em;\"><span>依托全过程人民民主<span style=\"font-size: 12pt;\">的深厚基础</span></span></p><p style=\"text-indent: 2em;\">全过程人民民主的深厚基础在于，要让人民广泛而深入地参与到民主协商、民主决策、民主管理和民主监督过程中，切实保证人民当家作主。民主立法的实践依托于这样的深厚基础。在民法典的编纂工作中，我们通过认真开展调查研究，广泛倾听民意，践行民主立法要求，将开门立法、问法于民贯穿于编纂工作的全过程。</p><p style=\"text-indent: 2em;\">五年来，共10次通过中国人大网公开征求社会公众意见，有42万多群众参与，提出102万多条意见，对这些意见，我们都进行认真梳理研究，并积极予以采纳。例如，根据公众的意见，增加了禁止高利放贷的规定，完善了预防、制止性骚扰的规定，增加了夫妻共同债务认定的规定，增加规定了收养评估制度等。</p><p style=\"text-indent: 2em;\">五年来，共27次赴地方开展立法调研，广鉴民情、汇聚民意、集中民智。我们还多次将草案印发中央有关部门、地方人大、有关研究机构、基层立法联系点等征求意见。</p><p style=\"text-indent: 2em;\">从调研和听取意见的方式上看：一是调研深入基层、深入群众，借助法工委设立的基层立法联系点，广泛听取基层群众的意见。编纂工作五年来，上海虹桥、江西景德镇、湖北襄阳、甘肃临洮这些基层立法联系点，作为国家立法机关联系基层群众的桥梁和纽带，都留下了常委会领导、法工委同志调研的足迹。对基层立法联系点提出的意见，我们都认真研究，积极采纳。如在湖北襄阳市人大常委会调研民法典婚姻家庭编时，他们提出了增加规定夫妻共同债务认定和收养评估制度的建议，得到了吸收采纳。二是综合调研与专题调研相结合，确保调研的针对性和实效性。不仅有全面了解情况的综合调研，还有专门针对农村集体经济组织的民事主体地位、夫妻共同债务、保理合同等各编重点问题的专题调研。三是委托专门机构开展专项调查。针对社会公众关心的法定婚龄问题，我们专门委托国家统计局社情民意调查中心对公民的结婚意向年龄开展调查，借助专业力量为立法决策提供了更为广泛的民意基础。四是在立法调研中积极采用问卷调查的方式，进一步提高调研质量。我们在广西、北京、浙江进行的三次调研中，针对居民小区管理、婚姻家庭继承制度等群众关心的问题，主动运用问卷调查配合开展调研，拓宽了解情况、收集意见的途径，取得了积极效果。</p><p style=\"text-indent: 2em;\"><span>展现全过程人民民主<span style=\"font-size: 12pt;\">的务实风范</span></span></p><p style=\"text-indent: 2em;\">习近平总书记指出：“民主不是装饰品，不是用来做摆设的，而是要用来解决人民需要解决的问题的。”全过程人民民主是真正的民主，是管用的民主，是回答现实问题、回应社会关切的民主。</p><p style=\"text-indent: 2em;\">随着经济社会的发展，人们的生产生活方式发生了深刻变化，给民事法律制度的发展完善带来了新的课题。在编纂民法典过程中，我们认真贯彻党中央的决策部署，深入分析研究法治建设中的新问题，积极回应人民群众的新期待，回答时代发展提出的新课题。例如，为解决民间借贷领域存在的突出问题，维护正常的金融秩序，在合同编中增加禁止高利放贷的规定；为解决人民群众反映强烈的小区管理问题，对物权编中的业主建筑物区分所有权制度作了进一步的完善，并在合同编中增加“物业服务合同”这一新的典型合同；为回应实践中出现的基因编辑，以及由于人工智能技术的发展带来的“换脸”“换声”等问题，在人格权编中增加了相应规定；为解决互联网时代的网络侵权问题，对相关侵权责任规定进行了完善等。</p><p style=\"text-indent: 2em;\">对社会公众比较关注的一些问题，如涉及婚姻家庭编的有关问题，我们也高度重视，通过法工委发言人制度平台，及时对外发声，回应关切，以解疑释惑，凝聚共识，为民法典编纂工作平稳顺利推进营造良好的舆论环境。2019年，通过法工委发言人平台，我们对媒体关注的有关热点问题作了积极回应，取得了很好的社会效果。2020年新冠肺炎疫情发生以来，我们认真学习贯彻习近平总书记的有关重要讲话精神，结合民法典编纂工作，一方面对与疫情相关的民事法律制度进行梳理研究，对草案作出有针对性的修改完善；另一方面针对社会关注的不可抗力等热点问题，通过法工委发言人主动发声，回应社会关切。</p><p style=\"text-indent: 2em;\">民法典编纂过程中的这些真招实干，展现出全过程人民民主的务实风范，得到了常委会组成人员、全国人大代表和各方面的总体认同。</p><p style=\"text-indent: 2em;\">2021年8月20日，栗战书委员长在第十三届全国人大常委会第三十次会议上的讲话中明确提出，发展全过程人民民主，是人大及其常委会的重要职责和使命。要在党的领导下，充分发挥人民代表大会这一主要民主渠道作用，把发展全过程人民民主的要求贯彻体现到人大各项工作中。全过程人民民主在民法典编纂工作中的体现，鲜活地阐释着我国全过程人民民主的原则理念、制度安排、生动实践和巨大成就，有助于我们进一步做到理论上清醒、政治上坚定、制度上自信、行动上自觉。民法典的中国故事还在继续，全过程人民民主的中国民主故事将更启新篇。（水淼\\u2002全国人大常委会法工委民法室）</p>'},\n",
       " 'published': 'Mon, 21 Mar 2022 15:16:21 GMT',\n",
       " 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=21, tm_hour=15, tm_min=16, tm_sec=21, tm_wday=0, tm_yday=80, tm_isdst=0),\n",
       " 'id': 'http://www.npc.gov.cn/npc/c30834/202203/dc92686f0b834dc3958eec84beacf81d.shtml',\n",
       " 'guidislink': False,\n",
       " 'links': [{'rel': 'alternate',\n",
       "   'type': 'text/html',\n",
       "   'href': 'http://www.npc.gov.cn/npc/c30834/202203/dc92686f0b834dc3958eec84beacf81d.shtml'}],\n",
       " 'link': 'http://www.npc.gov.cn/npc/c30834/202203/dc92686f0b834dc3958eec84beacf81d.shtml'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny['entries'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList, fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token] = fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedFreq[:30]\n",
    "\n",
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import jieba\n",
    "    listOfTokens = jieba.cut(bigString, cut_all = False) \n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def localWords(feed1, feed0):\n",
    "    import feedparser\n",
    "    docList = []; classList = []; fullText = []\n",
    "    minLen = min(len(feed1['entries']), len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    top30Words = calcMostFreq(vocabList, fullText)   #remove top 30 words\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = range(2*minLen); testSet = []           #create test set\n",
    "    for i in range(5):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(list(trainingSet)[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the error rate is: ', float(errorCount)/len(testSet))\n",
    "    return vocabList, p0V, p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\19035\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.864 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "sdm=feedparser.parse('https://rsshub.app/3dm/news') #3DM\n",
    "rd=feedparser.parse('https://rsshub.app/npc/c183') #中国人大网\n",
    "vocabList, p0V, p1V=localWords(sdm,rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopWords(ny, sf):\n",
    "    import operator\n",
    "    vocabList, p0V, p1V = localWords(ny, sf)\n",
    "    topNY = []; topSF = []\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0: topSF.append((vocabList[i], p0V[i]))\n",
    "        if p1V[i] > -6.0: topNY.append((vocabList[i], p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**\")\n",
    "    for item in sortedSF:\n",
    "        print(item[0])\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**\")\n",
    "    for item in sortedNY:\n",
    "        print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n",
      "sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**sdm**\n",
      "人大常委会\n",
      "委员会\n",
      "党中央\n",
      "民法典\n",
      "公司法\n",
      "2021\n",
      "十三届\n",
      "社会主义\n",
      "全国人大常委会\n",
      "黄河流域\n",
      "办事处\n",
      "现代化\n",
      "习近平\n",
      "依法治国\n",
      "管理工作\n",
      "全国人民代表大会\n",
      "修正案\n",
      "有利于\n",
      "当家作主\n",
      "充分发挥\n",
      "联系点\n",
      "保护法\n",
      "第十四届\n",
      "副委员长\n",
      "全国人民代表大会常务委员会\n",
      "一系列\n",
      "公共卫生\n",
      "建立健全\n",
      "实践经验\n",
      "党的领导\n",
      "征求意见\n",
      "全国人大常委会法工委\n",
      "安全法\n",
      "防治法\n",
      "国务院\n",
      "党和国家\n",
      "总书记\n",
      "依法行政\n",
      "贯彻落实\n",
      "政权机关\n",
      "促进法\n",
      "高度重视\n",
      "法工委\n",
      "法律委员会\n",
      "政权建设\n",
      "教育法\n",
      "管理法\n",
      "国有企业\n",
      "高质量\n",
      "经济社会\n",
      "区域合作\n",
      "规范性\n",
      "市场经济\n",
      "npc\n",
      "群众性\n",
      "明确要求\n",
      "合法权益\n",
      "2020\n",
      "国有资产\n",
      "针对性\n",
      "联络站\n",
      "基础性\n",
      "1979\n",
      "管理体制\n",
      "履行职责\n",
      "室主任\n",
      "第三十二\n",
      "社会公众\n",
      "法律法规\n",
      "普通教育\n",
      "自治区\n",
      "保障法\n",
      "直辖市\n",
      "行使职权\n",
      "中华人民共和国宪法\n",
      "市辖区\n",
      "水资源\n",
      "近年来\n",
      "主席团\n",
      "工作部门\n",
      "中共中央\n",
      "中国人民解放军\n",
      "个人信息\n",
      "密切联系\n",
      "2019\n",
      "全国政协\n",
      "事务所\n",
      "新华社\n",
      "市场化\n",
      "代表团\n",
      "民族乡\n",
      "人代会\n",
      "font\n",
      "高等学校\n",
      "全方位\n",
      "市场主体\n",
      "产权保护\n",
      "rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**rd**\n",
      "png\n",
      "strong\n",
      "扭蛋机\n",
      "马斯克\n",
      "皮克斯\n",
      "动画电影\n",
      "特别版\n",
      "地下城\n",
      "1648436353\n",
      "直升机\n",
      "第二次\n",
      "亿美元\n",
      "playstation\n",
      "ultra\n",
      "最终幻想\n",
      "不可思议\n",
      "工作室\n",
      "talk\n",
      "coffee\n",
      "snail\n",
      "rogue\n",
      "amp\n",
      "div\n",
      "周年纪念\n",
      "windows\n",
      "switch\n",
      "博物馆\n",
      "亿万富翁\n",
      "target\n",
      "南梦宫\n",
      "ios\n",
      "消费者\n",
      "href\n",
      "智能手机\n",
      "blank\n",
      "ps4\n",
      "pac\n",
      "1648445790\n",
      "fahmi\n",
      "1500\n",
      "www.3\n",
      "符合事实\n",
      "3999\n",
      "1648435709\n",
      "500\n",
      "6500\n",
      "2022\n",
      "greg\n",
      "1648437090\n",
      "数百米\n",
      "xbox\n",
      "miller\n",
      "1648437557\n",
      "马铃薯\n",
      "2020\n",
      "单行本\n",
      "lucas\n",
      "慈善事业\n",
      "一部分\n",
      "games\n",
      "tag\n",
      "创始人\n",
      "odyssey\n",
      "app\n",
      "one\n",
      "创作者\n",
      "开发商\n",
      "1.0\n",
      "gashapon\n",
      "1000\n",
      "7.6\n",
      "dma\n",
      "1648445779\n",
      "cross\n",
      "联合国\n",
      "67w\n",
      "二十周年\n",
      "特斯拉\n",
      "ceo\n",
      "主题歌\n",
      "productions\n",
      "48mp\n",
      "store\n",
      "第三方\n",
      "2023\n",
      "martinger\n",
      "nbsp\n",
      "edge\n",
      "1648439188\n",
      "位图文件\n",
      "亚马逊\n",
      "the\n",
      "推特上\n",
      "亿万富豪\n",
      "daniel\n",
      "显示器\n",
      "windows1.0\n",
      "toge\n",
      "微博热\n",
      "netflix\n",
      "brook\n",
      "曾多次\n",
      "格斗游戏\n",
      "美国政府\n",
      "steam\n",
      "第三代\n",
      "手机游戏\n",
      "总动员\n",
      "计划署\n",
      "1648437089\n",
      "1648445780\n",
      "ps5\n",
      "集英社\n",
      "1648435553\n",
      "标志性\n",
      "应用程序\n"
     ]
    }
   ],
   "source": [
    "getTopWords(sdm,rd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
